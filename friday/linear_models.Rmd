---
title: "A Brief Introduction to Linear Models in R"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 0. Introduction
Many bioinformatics applications involving repeatedly fitting linear models to data.  Examples include:

* RNA-Seq differential expression analyses
* GWAS (for continuous traits)
* eQTL analyses
* Microarray data analyses

Understanding linear modelling in R can help in implementing these types of analyses.

## Scope
* Basics of linear models
* R model syntax
* Understanding contrasts
* Models with continuous covariates

We will not discuss:

* Diagnostic plots 
* Data-driven model selection
* Anything that doesn't scale well when applied to 1000's of genes/SNPs/proteins

# 1. Linear models
A linear model is a model for a continuous outcome Y of the form
$$Y = \beta_0 + \beta_{1}X_{1} + \beta_{2}X_{2} + \dots + \beta_{p}X_{p} + \epsilon$$
The covariates X can be:

* a continuous variable (age, weight, temperature, etc.)
* Dummy variables coding a categorical covariate (more later)

The $\beta$'s are unknown parameters to be estimated.

The error term $\epsilon$ is assumed to be normally distributed with a variance that is constant across the range of the data.

Models with all categorical covariates are referred to as ANOVA models and models with continuous covariates are referred to as linear regression models. These are all linear models, and R doesn't distinguish between them.

# 2. Linear models in R
R uses the function `lm` to fit linear models.

Read in 'lm_example_data.csv`:
```{r}
dat <- read.csv("https://raw.githubusercontent.com/ucdavis-bioinformatics-training/2018-September-Bioinformatics-Prerequisites/master/friday/lm_example_data.csv")
head(dat)
str(dat)
```

Fit a linear model using `expression` as the outcome and `treatment` as a categorical covariate:
```{r}
oneway.model <- lm(expression ~ treatment, data = dat)
```
In R model syntax, the outcome is on the left side, with covariates (separated by `+`) following the `~`
```{r}
oneway.model
class(oneway.model)
```
Note that this is a one-way ANOVA model.


`summary()` applied to an `lm` object will give p-values and other relevant information:
```{r}
summary(oneway.model)
```
In the output:

* "Coefficients" refer to the $\beta$'s
* "Estimate" is the estimate of each coefficient
* "Std. Error" is the standard error of the estimate
* "t value" is the coefficient divided by its standard error
* "Pr(>|t|)" is the p-value for the coefficient
* The residual standard error is the estimate of the variance of $\epsilon$
* Degrees of freedom is the sample size minus # of coefficients estimated
* R-squared is (roughly) the proportion of variance in the outcome explained by the model
* The F-statistic compares the fit of the model _as a whole_ to the null model (with no covariates)

`coef()` gives you model coefficients:
```{r}
coef(oneway.model)
```
What do the model coefficients mean?

By default, R uses reference group coding or "treatment contrasts". For categorical covariates, the first level alphabetically (or first factor level) is treated as the reference group.  The reference group doesn't get its own coefficient, it is represented by the intercept.  Coefficients for other groups are the difference from the reference:

For our simple design:

* `(Intercept)` is the mean of expression for treatment  = A
* `treatmentB` is the mean of expression for treatment = B minus the mean for treatment = A
* `treatmentC` is the mean of expression for treatment = C minus the mean for treatment = A
* etc.
```{r}
# Get means in each treatment
treatmentmeans <- tapply(dat$expression, dat$treatment, mean)
treatmentmeans["A"] 
# Difference in means gives you the "treatmentB" coefficient from oneway.model
treatmentmeans["B"] - treatmentmeans["A"] 
```

What if you don't want reference group coding?  Another option is to fit a model without an intercept:
```{r}
no.intercept.model <- lm(expression ~ 0 + treatment, data = dat) # '0' means 'no intercept' here
summary(no.intercept.model)
coef(no.intercept.model)
```
Without the intercept, the coefficients here estimate the mean in each level of treatment:
```{r}
treatmentmeans
```
The no-intercept model is the SAME model as the reference group coded model, in the sense that it gives the same estimate for any comparison between groups:

Treatment B - treatment A, reference group coded model:
```{r}
coefs <- coef(oneway.model)
coefs["treatmentB"]
```
Treatment B - treatment A, no-intercept model:
```{r}
coefs <- coef(no.intercept.model)
coefs["treatmentB"] - coefs["treatmentA"]
```

## The Design Matrix
For the RNASeq analysis programs `limma` and `edgeR`, the model is specified through the _design matrix_.

The design matrix $\mathbf{X}$ has one row for each observation and one column for each model coefficient.

Sound complicated?  The good news is that the design matrix can be specified through the `model.matrix` function using the same syntax as for `lm`, just without a response:

Design matrix for reference group coded model:
```{r}
X <- model.matrix(~treatment, data = dat)
X
```
(Note that "contr.treatment", or treatment contrasts, is how R refers to reference group coding)

* The first column will always be 1 in every row if your model has an intercept
* The column `treatmentB` is 1 if an observation has treatment B and 0 otherwise
* The column `treatmentC` is 1 if an observation has treatment C and 0 otherwise
* etc.

## Exercises and Things to Think About
- Use ?formula to explore specifying models in R.  
- Use ?lm.fit to see how lm uses the design matrix internally.
- If the response y is log gene expression, the model coefficients are often referred to as log fold-changes.  Why does this make sense?  (Hint: log(x/y) = log(x) - log(y))

# 3. Adding More Covariates
## Batch Adjustment
Suppose we want to adjust for batch differences in our model.  We do this by adding the covariate "batch" to the model formula:
```{r}
batch.model <- lm(expression ~ treatment + batch, data = dat)
summary(batch.model)
coef(batch.model)
```
For a model with more than one coefficient, `summary` provides estimates and tests for each coefficient adjusted for all the other coefficients in the model.

## Two-Way ANOVA Models
Suppose our experiment involves two factors, treatment and time.  `lm` can be used to fit a two-way ANOVA model:
```{r}
twoway.model <- lm(expression ~ treatment*time, data = dat)
summary(twoway.model)
coef(twoway.model)
```
The notation `treatment*time` refers to treatment, time, and the interaction effect of treatment by time.  (This is different from other statistical software).

Interpretation of coefficients:

* Each coefficient for treatment represents the difference between the indicated group and the reference group _at the reference level for the other covariates_ 
* For example, "treatmentB" is the difference in expression between treatment B and treatment A at time 1
* Similarly, "timetime2" is the difference in expression between time2 and time1 for treatment A
* The interaction effects (coefficients with ":") estimate the difference between treatment groups in the effect of time
* The interaction effects ALSO estimate the difference between times in the effect of treatment

To estimate the difference between treatment B and treatment A at time 2, we need to include the interaction effects:
```{r}
# A - B at time 2
coefs <- coef(twoway.model)
coefs["treatmentB"] + coefs["treatmentB:timetime2"]
```
We can see from `summary` that one of the interaction effects is significant.  Here's what that interaction effect looks like graphically:
```{r}
interaction.plot(x.factor = dat$time, trace.factor = dat$treatment, response = dat$expression)
```

#### Another Parameterization
In a multifactor model, estimating contrasts can be fiddly, especially with lots of factors or levels.  Here is an equivalent way to estimate the same two-way ANOVA model that gives easier contrasts:

First, define a new variable that combines the information from the `treatment` and `time` variables
```{r}
dat$tx.time <- interaction(dat$treatment, dat$time)
dat$tx.time
table(dat$tx.time, dat$treatment)
table(dat$tx.time, dat$time)
```
Next, fit a one-way ANOVA model with the new covariate.  Don't include an intercept in the model.
```{r}
other.2way.model <- lm(expression ~ 0 + tx.time, data = dat)
summary(other.2way.model)
coef(other.2way.model)
```
We get the same estimates for the effect of treatment B vs. A at time 1:
```{r}
c1 <- coef(twoway.model)
c1["treatmentB"] 
c2 <- coef(other.2way.model)
c2["tx.timeB.time1"] - c2["tx.timeA.time1"]
```
We get the same estimates for the effect of treatment B vs. A at time 2:
```{r}
c1 <- coef(twoway.model)
c1["treatmentB"] + c1["treatmentB:timetime2"]
c2 <- coef(other.2way.model)
c2["tx.timeB.time2"] - c2["tx.timeA.time2"]
```
And we get the same estimates for the interaction effect (remembering that an interaction effect here is a difference of differences):
```{r}
c1 <- coef(twoway.model)
c1["treatmentB:timetime2"]
c2 <- coef(other.2way.model)
(c2["tx.timeB.time2"] - c2["tx.timeA.time2"]) - (c2["tx.timeB.time1"] - c2["tx.timeA.time1"])
```

(See 
https://www.bioconductor.org/packages/3.7/bioc/vignettes/limma/inst/doc/usersguide.pdf
for more details on this parameterization)

## Exercises and Things to Think About
- How much do the parameter estimates for treatment change when batch is added?  
- The data frame dat has a column called 'temperature'.  What formula would you use if you wanted to look at differences between treatments, adjusting for temperature? 

# 4. Continuous Covariates
Linear models with continuous covariates ("regression models") are fitted in much the same way:
```{r}
continuous.model <- lm(expression ~ temperature, data = dat)
summary(continuous.model)
coef(continuous.model)
```
For the above model, the intercept is the expression at temperature 0 and the "temperature" coefficient is the slope, or how much expression increases for each unit increase in temperature:
```{r}
coefs <- coef(continuous.model)
plot(expression ~ temperature, data = dat)
abline(coefs)
text(x = 12, y = 10, paste0("expression = ", round(coefs[1], 2),  "+", round(coefs[2], 2), "*temperature"))
```

The slope from a linear regression model is related to but not identical to the Pearson correlation coefficient:
```{r}
cor.test(dat$expression, dat$temperature)
summary(continuous.model)
```
Notice that the p-values for the correlation and the regression slope are identical.

Scaling and centering both variables yields a regression slope equal to the correlation coefficient:
```{r}
scaled.mod <- lm(scale(expression) ~ scale(temperature), data = dat)
coef(scaled.mod)[2]
cor(dat$expression, dat$temperature)
```

## Exercises and things to think about
- Look at the documentation for formula again using ?formula.  How would you change the formula statement if you wanted to add a quadratic term?
- Convert temperature to Farenheit by replacing temperature with I(9/5*temperature + 32) in the model formula.  Does the p-value for the association with expression change?
- *For your experiment, what would the model formula look like?*













